## 健康检查

### introduce

1. 本质: `是个定时器, 定期检查指定 upstream 组的状态(endpoint + 历史请求记录[内存中记录-_M.status_page()])`
2. 如果状态发生改变则在**共享内存**更新记录, 下次执行时所有 worker 进程都要更新到最新的 peer 状态
   - 在下一次定时任务执行时, 所有 worker 进程**主动**首先去共享内存中查取
3. 配置

   ```conf
   http {
       upstream backend {
           server backend1.example.com;
           server backend2.example.com;

           healthcheck interval=5s fall=3 rise=2 timeout=2s type=http;
       }

       server {
           location / {  proxy_pass http://backend;   }
           # 响应健康检查的请求: 200 为正常
            location = /status {
                access_log off;
                allow 127.0.0.1;
                deny all;

                default_type text/plain;
                content_by_lua_block {
                    local hc = require "resty.upstream.healthcheck"
                    ngx.say("Nginx Worker PID: ", ngx.worker.pid())
                    ngx.print(hc.status_page()) // 数组
                }
            }
       }
   }
   ```

   - interval: 设置健康检查的间隔时间, 默认为 5 秒
   - fall: 指定连续失败的次数达到多少次时认为后端服务器不可用, 默认为 3 次
   - rise: 指定连续成功的次数达到多少次时认为后端服务器可用, 默认为 2 次
   - timeout: 设置健康检查的超时时间, 默认为 2 秒
   - type: 设置健康检查的类型, 支持 http、https、tcp 和 custom 四种类型
   - uri: 健康检查的端点

### 实现

1. init_worker_by_lua_block

   ```lua
   local hc = require "resty.upstream.healthcheck"
   local ok, err = hc.spawn_checker{
       shm = "healthcheck",  -- defined by "lua_shared_dict"
       upstream = "foo.com", -- defined by "upstream"
       type = "http",

       http_req = "GET /status HTTP/1.0\r\nHost: foo.com\r\n\r\n", -- raw HTTP request for checking
       interval = 2000,  -- run the check cycle every 2 sec
       timeout = 1000,   -- 1 sec is the timeout for network operations
       fall = 3,  -- # of successive failures before turning a peer down
       rise = 2,  -- # of successive successes before turning a peer up
       valid_statuses = {200, 302},  -- a list valid HTTP status code
       concurrency = 10,  -- concurrency level for test requests
   }
   if not ok then
       ngx.log(ngx.ERR, "failed to spawn health checker: ", err)
       return
   end
   ```

2. spawn_checker: 检测的是请求而不是端口(防止假死) + 并发线程执行多个检查任务

   ![avatar](/static/image/openresty/openresty-hc-overview.png)
   ![avatar](/static/image/openresty/openresty-hc-checker.png)

3. 怎么实现服务状态更改: 共享内存

   - 成功和失败会分为两个带版本号的变量存储在**共享内存**中: ok:xx_stream:p9
   - 只有一个 worker 会**争抢**到执行的轻量级 lua 脚本的机会[多个 ups 则会多线程执行]: 发送 http 请求到后端
   - 探测结果成功, 会获取共享内存的数据进行+1 后存回共享内存
   - 判断是都达到阈值, 是则修改全局的服务状态

4. 怎么实现不同 worker 进程间的服务状态同步: 版本控制

   - 维护全局的 peer 状态: **版本控制**(专门在共享内存中记录专门的记录来传递-抢占式的 ctx 上的 version 不连续)
   - 在下一次定时任务执行时, 所有 worker 进程首先**主动**去共享内存中查找 key 得到服务状态, 设置自己 worker 的服务状态

## reference

1. https://blog.csdn.net/tao_627/article/details/79298904
2. https://blog.csdn.net/lgxzzz/article/details/121683302
